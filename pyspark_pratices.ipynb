{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d2994423-1c4d-416c-b42e-0da5cafd7bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "036a940e-c807-4096-bb83-8a864d1a968e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+\n|actor_id|director_id|timestamp|\n+--------+-----------+---------+\n|       1|          1|        0|\n|       1|          1|        1|\n|       1|          1|        2|\n|       1|          2|        3|\n|       1|          2|        4|\n|       2|          1|        5|\n|       2|          1|        6|\n+--------+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 1, 0),\n",
    "    (1, 1, 1),\n",
    "    (1, 1, 2),\n",
    "    (1, 2, 3),\n",
    "    (1, 2, 4),\n",
    "    (2, 1, 5),\n",
    "    (2, 1, 6)\n",
    "]\n",
    "columns = [\"actor_id\", \"director_id\", \"timestamp\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f42373-d734-40ee-859c-8f4d0364cb9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+\n|actor_id|director_id|count|\n+--------+-----------+-----+\n|       1|          1|    3|\n+--------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Group by actor_id and director_id, calculate the count, and filter\n",
    "df = df.groupBy(\"actor_id\", \"director_id\") \\\n",
    "       .agg(count(\"*\").alias(\"count\")) \\\n",
    "       .filter(\"count > 2\")  # OR use filter(F.col(\"count\") > 2)\n",
    "\n",
    "# Show the result\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296be89c-3b31-430c-8747-d7ba4dce968b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Write an SPARK query to find all the authors that viewed at least one of their own articles, sorted in ascending order by their id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a1ce9c20-bde4-46c6-8104-9b6dcbb11008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>article_id</th><th>author_id</th><th>viewer_id</th><th>view_date</th></tr></thead><tbody><tr><td>1</td><td>3</td><td>5</td><td>2019-08-01</td></tr><tr><td>1</td><td>3</td><td>6</td><td>2019-08-02</td></tr><tr><td>2</td><td>7</td><td>7</td><td>2019-08-01</td></tr><tr><td>2</td><td>7</td><td>6</td><td>2019-08-02</td></tr><tr><td>4</td><td>7</td><td>1</td><td>2019-07-22</td></tr><tr><td>3</td><td>4</td><td>4</td><td>2019-07-21</td></tr><tr><td>3</td><td>4</td><td>4</td><td>2019-07-21</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         3,
         5,
         "2019-08-01"
        ],
        [
         1,
         3,
         6,
         "2019-08-02"
        ],
        [
         2,
         7,
         7,
         "2019-08-01"
        ],
        [
         2,
         7,
         6,
         "2019-08-02"
        ],
        [
         4,
         7,
         1,
         "2019-07-22"
        ],
        [
         3,
         4,
         4,
         "2019-07-21"
        ],
        [
         3,
         4,
         4,
         "2019-07-21"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "article_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "author_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "viewer_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "view_date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 3, 5, \"2019-08-01\"),\n",
    "    (1, 3, 6, \"2019-08-02\"),\n",
    "    (2, 7, 7, \"2019-08-01\"),\n",
    "    (2, 7, 6, \"2019-08-02\"),\n",
    "    (4, 7, 1, \"2019-07-22\"),\n",
    "    (3, 4, 4, \"2019-07-21\"),\n",
    "    (3, 4, 4, \"2019-07-21\"),\n",
    "]\n",
    "\n",
    "columns = [\"article_id\", \"author_id\", \"viewer_id\", \"view_date\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "views_df = spark.createDataFrame(data, columns)\n",
    "views_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d1888544-295d-4463-8625-d25d34c89e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>author_id</th></tr></thead><tbody><tr><td>7</td></tr><tr><td>4</td></tr><tr><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7
        ],
        [
         4
        ],
        [
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "author_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "views_df = views_df.select(\"author_id\").filter(\"author_id == viewer_id\")\n",
    "views_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99a0024a-ce20-4cc7-b459-010eb6e8f440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Write an SPark query to find the average selling price for each product.\n",
    "\n",
    " average_price should be rounded to 2 decimal places.\n",
    "\n",
    " The query result format is in the following example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2caeeb70-ca94-4afa-9e38-280a68fd3df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----+\n|product_id|start_date|  end_date|price|\n+----------+----------+----------+-----+\n|         1|2019-02-17|2019-02-28|    5|\n|         1|2019-03-01|2019-03-22|   20|\n|         2|2019-02-01|2019-02-20|   15|\n|         2|2019-02-21|2019-03-31|   30|\n+----------+----------+----------+-----+\n\n+----------+-------------+-----+\n|product_id|purchase_date|units|\n+----------+-------------+-----+\n|         1|   2019-02-25|  100|\n|         1|   2019-03-01|   15|\n|         2|   2019-02-10|  200|\n|         2|   2019-03-22|   30|\n+----------+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Prices and UnitsSold\").getOrCreate()\n",
    "\n",
    "# Define the schema for the Prices table\n",
    "prices_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"start_date\", DateType(), True),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Convert string dates to datetime.date objects\n",
    "prices_data = [\n",
    "    (1, datetime.strptime(\"2019-02-17\", \"%Y-%m-%d\").date(), datetime.strptime(\"2019-02-28\", \"%Y-%m-%d\").date(), 5),\n",
    "    (1, datetime.strptime(\"2019-03-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2019-03-22\", \"%Y-%m-%d\").date(), 20),\n",
    "    (2, datetime.strptime(\"2019-02-01\", \"%Y-%m-%d\").date(), datetime.strptime(\"2019-02-20\", \"%Y-%m-%d\").date(), 15),\n",
    "    (2, datetime.strptime(\"2019-02-21\", \"%Y-%m-%d\").date(), datetime.strptime(\"2019-03-31\", \"%Y-%m-%d\").date(), 30)\n",
    "]\n",
    "\n",
    "# Create Prices DataFrame\n",
    "prices_df = spark.createDataFrame(prices_data, schema=prices_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "prices_df.show()\n",
    "\n",
    "# Define the schema for the UnitsSold table\n",
    "units_sold_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"purchase_date\", DateType(), True),\n",
    "    StructField(\"units\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Convert string dates to datetime.date objects\n",
    "units_sold_data = [\n",
    "    (1, datetime.strptime(\"2019-02-25\", \"%Y-%m-%d\").date(), 100),\n",
    "    (1, datetime.strptime(\"2019-03-01\", \"%Y-%m-%d\").date(), 15),\n",
    "    (2, datetime.strptime(\"2019-02-10\", \"%Y-%m-%d\").date(), 200),\n",
    "    (2, datetime.strptime(\"2019-03-22\", \"%Y-%m-%d\").date(), 30)\n",
    "]\n",
    "\n",
    "# Create UnitsSold DataFrame\n",
    "units_sold_df = spark.createDataFrame(units_sold_data, schema=units_sold_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "units_sold_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c7dbf1-f17b-44de-9065-f719eac0f456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19aa350a-7c68-4631-853d-3b719f6fa716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------+\n|product_id|(sum((units * price)) / sum(units))|\n+----------+-----------------------------------+\n|         1|                                5.0|\n|         2|                 16.956521739130434|\n+----------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_new=prices_df.join(units_sold_df, on=\"product_id\" ).filter((col(\"purchase_date\")>col(\"start_date\")) & (col(\"purchase_date\")<col(\"end_date\")))\n",
    "df_new=df_new.groupBy(\"product_id\").agg(sum(col(\"units\")*col(\"price\"))/sum(\"units\"))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0088cb6-5d78-4d15-852d-f97f5022a87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Write a Pyspark solution to output big countries' name, population and area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "326ab98a-9349-4407-a6ea-d61302224bf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+----------+---------+\n|       name|continent|   area|population|      gdp|\n+-----------+---------+-------+----------+---------+\n|Afghanistan|     Asia| 652230|  25500100| 20343000|\n|    Albania|   Europe|  28748|   2831741| 12960000|\n|    Algeria|   Africa|2381741|  37100000|188681000|\n|    Andorra|   Europe|    468|     78115|  3712000|\n|     Angola|   Africa|1246700|  20609294|100990000|\n+-----------+---------+-------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "world_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"area\", IntegerType(), True),\n",
    "    StructField(\"population\", IntegerType(), True),\n",
    "    StructField(\"gdp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for the World table\n",
    "world_data = [\n",
    "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000),\n",
    "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000),\n",
    "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000),\n",
    "    (\"Andorra\", \"Europe\", 468, 78115, 3712000),\n",
    "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000)\n",
    "]\n",
    "\n",
    "# Create World DataFrame\n",
    "world_df = spark.createDataFrame(world_data, schema=world_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "world_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ec7c9b-af03-4631-a9a1-a2482e1da62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n|       name|population|\n+-----------+----------+\n|Afghanistan|  25500100|\n|    Algeria|  37100000|\n+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "world_df=world_df.select(\"name\",\"population\").filter((col(\"area\")>3000000) | (col(\"population\")>25000000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2928ad0-e16b-496d-835e-f2f59e62b375",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Can you write a SQL query to find the biggest number, which only appears once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15668667-ae76-4a3f-a22f-122932891b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data\n",
    "data = [(8,), (8,), (3,), (3,), (1,), (4,), (5,), (6,)]\n",
    "columns = [\"num\"]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55f874a-6076-4682-9ea6-818aecf5f204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n|num|count|\n+---+-----+\n|  6|    1|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df = df.groupBy(\"num\").agg(count(\"*\").alias(\"count\")).filter(col(\"count\")==1).orderBy(\"num\",ascending=False).limit(1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dee4bae-425a-475e-bf35-595d49f4dc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There is a table courses with columns: student and class\n",
    "\n",
    "-- Please list out all classes which have more than or equal to 5 students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "39f507a3-31da-46d8-9487-23452c833737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n|student|   class|\n+-------+--------+\n|      A|    Math|\n|      B| English|\n|      C|    Math|\n|      D| Biology|\n|      E|    Math|\n|      F|Computer|\n|      G|    Math|\n|      H|    Math|\n|      I|    Math|\n+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrameExample\").getOrCreate()\n",
    "\n",
    "# Define the data as a list of tuples\n",
    "data = [\n",
    "    (\"A\", \"Math\"),\n",
    "    (\"B\", \"English\"),\n",
    "    (\"C\", \"Math\"),\n",
    "    (\"D\", \"Biology\"),\n",
    "    (\"E\", \"Math\"),\n",
    "    (\"F\", \"Computer\"),\n",
    "    (\"G\", \"Math\"),\n",
    "    (\"H\", \"Math\"),\n",
    "    (\"I\", \"Math\"),\n",
    "]\n",
    "\n",
    "# Define the schema (column names)\n",
    "columns = [\"student\", \"class\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8855ebbb-cd42-4097-a457-e37972154295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n|class|count|\n+-----+-----+\n| Math|    6|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").agg(count(\"*\").alias(\"count\")).filter(\"count>5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "435e0afe-7ff3-4a17-a24d-7e6d52be3fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write a SQL query for a report that provides the following information for each person in the Person table,\n",
    "-- regardless if there is an address for each of those people:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5ebe347f-eaa3-4020-a523-ada2eb77f5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person DataFrame:\n+--------+---------+--------+\n|PersonId|FirstName|LastName|\n+--------+---------+--------+\n|       1|     John|     Doe|\n|       2|     Jane|   Smith|\n|       3|      Sam|   Brown|\n|       4|     Lisa|   White|\n+--------+---------+--------+\n\nAddress DataFrame:\n+---------+--------+-----------+-----+\n|AddressId|PersonId|       City|State|\n+---------+--------+-----------+-----+\n|        1|       1|   New York|   NY|\n|        2|       2|Los Angeles|   CA|\n|        3|       4|    Chicago|   IL|\n+---------+--------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrameExample\").getOrCreate()\n",
    "\n",
    "# Data for the Person table\n",
    "person_data = [\n",
    "    (1, \"John\", \"Doe\"),\n",
    "    (2, \"Jane\", \"Smith\"),\n",
    "    (3, \"Sam\", \"Brown\"),\n",
    "    (4, \"Lisa\", \"White\")\n",
    "]\n",
    "\n",
    "# Schema for the Person table\n",
    "person_columns = [\"PersonId\", \"FirstName\", \"LastName\"]\n",
    "\n",
    "# Create Person DataFrame\n",
    "person_df = spark.createDataFrame(person_data, person_columns)\n",
    "\n",
    "# Data for the Address table\n",
    "address_data = [\n",
    "    (1, 1, \"New York\", \"NY\"),\n",
    "    (2, 2, \"Los Angeles\", \"CA\"),\n",
    "    (3, 4, \"Chicago\", \"IL\")\n",
    "]\n",
    "\n",
    "# Schema for the Address table\n",
    "address_columns = [\"AddressId\", \"PersonId\", \"City\", \"State\"]\n",
    "\n",
    "# Create Address DataFrame\n",
    "address_df = spark.createDataFrame(address_data, address_columns)\n",
    "\n",
    "# Show the DataFrames\n",
    "print(\"Person DataFrame:\")\n",
    "person_df.show()\n",
    "\n",
    "print(\"Address DataFrame:\")\n",
    "address_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5a71c963-060e-4f2d-9e2e-dc6f818661c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|FirstName|\n+---------+\n|     John|\n|     Jane|\n|     Lisa|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_pa=person_df.join(address_df,address_df.PersonId==person_df.PersonId,\"inner\").select(\"FirstName\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5772b910-c506-40e3-81ef-a653a0028cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Several friends at a cinema ticket office would like to reserve consecutive available seats.\n",
    "-- Can you help to query all the consecutive available seats order by the seat_id using the following cinema table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f969babb-532f-4d68-b7a6-9dfff0192488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+----+\n|seat_id|free|lead| lag|\n+-------+----+----+----+\n|      1|   1|   0|NULL|\n|      2|   0|   1|   1|\n|      3|   1|   1|   0|\n|      4|   1|   1|   1|\n|      5|   1|NULL|   1|\n+-------+----+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, lag\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"LeadLagExample\").getOrCreate()\n",
    "\n",
    "# Sample data for the cinema table\n",
    "data = [\n",
    "    (1, 1),\n",
    "    (2, 0),\n",
    "    (3, 1),\n",
    "    (4, 1),\n",
    "    (5, 1)\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"seat_id\", \"free\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a window specification ordered by seat_id\n",
    "window_spec = Window.orderBy(\"seat_id\")\n",
    "\n",
    "# Add 'lead' and 'lag' columns\n",
    "new_df = (\n",
    "    df.withColumn(\"lead\", lead(\"free\", 1).over(window_spec))  # Adds next row's value\n",
    "       .withColumn(\"lag\", lag(\"free\", 1).over(window_spec))   # Adds previous row's value\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b6ff53-5d6c-4bdd-9f33-71b573b2fade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+---+\n|seat_id|free|lead|lag|\n+-------+----+----+---+\n|      4|   1|   1|  1|\n|      5|   1|NULL|  1|\n+-------+----+----+---+\n\n"
     ]
    }
   ],
   "source": [
    "consecutive_seats = new_df.filter((new_df.free == 1) & (new_df.lag == 1))\n",
    "consecutive_seats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51388221-c1ff-4ed6-a535-03baaccb26d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query the customer_number from the orders table for the customer who has placed the largest number of orders.\n",
    "\n",
    "-- It is guaranteed that exactly one customer will have placed more orders than any other customer.\n",
    "\n",
    "-- The orders table is defined as follows:\n",
    "\n",
    "-- | Column            | Type      |\n",
    "-- |-------------------|-----------|\n",
    "-- | order_number (PK) | int       |\n",
    "-- | customer_number   | int       |\n",
    "-- | order_date        | date      |\n",
    "-- | required_date     | date      |\n",
    "-- | shipped_date      | date      |\n",
    "-- | status            | char(15)  |\n",
    "-- | comment           | char(200) |\n",
    "-- Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ed276b-68d7-4f1e-a8f9-68ce2c5f2eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+----------+-------------+------------+------+-------+\n|order_number|customer_number|order_date|required_date|shipped_date|status|comment|\n+------------+---------------+----------+-------------+------------+------+-------+\n|           1|              1|2017-04-09|   2017-04-13|  2017-04-12|Closed|       |\n|           2|              2|2017-04-15|   2017-04-20|  2017-04-18|Closed|       |\n|           3|              3|2017-04-16|   2017-04-25|  2017-04-20|Closed|       |\n|           4|              3|2017-04-18|   2017-04-28|  2017-04-25|Closed|       |\n+------------+---------------+----------+-------------+------------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
    "\n",
    "# Sample data for the orders table\n",
    "data = [\n",
    "    (1, 1, \"2017-04-09\", \"2017-04-13\", \"2017-04-12\", \"Closed\", \"\"),\n",
    "    (2, 2, \"2017-04-15\", \"2017-04-20\", \"2017-04-18\", \"Closed\", \"\"),\n",
    "    (3, 3, \"2017-04-16\", \"2017-04-25\", \"2017-04-20\", \"Closed\", \"\"),\n",
    "    (4, 3, \"2017-04-18\", \"2017-04-28\", \"2017-04-25\", \"Closed\", \"\")\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"order_number\", \"customer_number\", \"order_date\", \"required_date\", \"shipped_date\", \"status\", \"comment\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7d0c74-96ad-4cb5-9260-b2368051f916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n|customer_number|count|\n+---------------+-----+\n|              3|    2|\n+---------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.groupBy(\"customer_number\").agg(count(\"*\").alias(\"count\")).orderBy(\"count\",ascending=False ).limit(1).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb5169cd-ee23-4d65-9fe4-9c8e9af7e56c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Question 13\n",
    "-- Suppose that a website contains two tables, \n",
    "-- the Customers table and the Orders table. Write a SQL query to find all customers who never order anything.\n",
    "\n",
    "-- Table: Customers.\n",
    "\n",
    "-- +----+-------+\n",
    "-- | Id | Name  |\n",
    "-- +----+-------+\n",
    "-- | 1  | Joe   |\n",
    "-- | 2  | Henry |\n",
    "-- | 3  | Sam   |\n",
    "-- | 4  | Max   |\n",
    "-- +----+-------+\n",
    "-- Table: Orders.\n",
    "\n",
    "-- +----+------------+\n",
    "-- | Id | CustomerId |\n",
    "-- +----+------------+\n",
    "-- | 1  | 3          |\n",
    "-- | 2  | 1          |\n",
    "-- +----+------------+\n",
    "-- Using the above tables as example, return the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b315a5e-3e3f-440f-958e-96601f071e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers DataFrame:\n+---+-----+\n| Id| Name|\n+---+-----+\n|  1|  Joe|\n|  2|Henry|\n|  3|  Sam|\n|  4|  Max|\n+---+-----+\n\nOrders DataFrame:\n+---+----------+\n| Id|CustomerId|\n+---+----------+\n|  1|         3|\n|  2|         1|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreateCustomersOrdersDataFrame\").getOrCreate()\n",
    "\n",
    "# Sample data for Customers and Orders tables\n",
    "customers_data = [\n",
    "    (1, \"Joe\"),\n",
    "    (2, \"Henry\"),\n",
    "    (3, \"Sam\"),\n",
    "    (4, \"Max\")\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    (1, 3),\n",
    "    (2, 1)\n",
    "]\n",
    "\n",
    "# Define the schema for Customers and Orders DataFrames\n",
    "customers_columns = [\"Id\", \"Name\"]\n",
    "orders_columns = [\"Id\", \"CustomerId\"]\n",
    "\n",
    "# Create DataFrames\n",
    "customers_df = spark.createDataFrame(customers_data, customers_columns)\n",
    "orders_df = spark.createDataFrame(orders_data, orders_columns)\n",
    "\n",
    "# Show the DataFrames\n",
    "print(\"Customers DataFrame:\")\n",
    "customers_df.show()\n",
    "\n",
    "print(\"Orders DataFrame:\")\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84d2eb8-8599-4669-a9d2-97e0662472b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| Name|\n+-----+\n|Henry|\n|  Max|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "customers_df \\\n",
    "    .join(orders_df, customers_df.Id == orders_df.CustomerId, \"left_outer\") \\\n",
    "    .filter(orders_df.CustomerId.isNull()) \\\n",
    "    .select(\"Name\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "529613db-65db-446b-bd57-fd0cf36b0471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write a SQL query to delete all duplicate email entries in a table named Person, keeping only unique emails based on its smallest Id.\n",
    "\n",
    "-- +----+------------------+\n",
    "-- | Id | Email            |\n",
    "-- +----+------------------+\n",
    "-- | 1  | john@example.com |\n",
    "-- | 2  | bob@example.com  |\n",
    "-- | 3  | john@example.com |\n",
    "-- +----+------------------+\n",
    "-- Id is the primary key column for this table.\n",
    "-- For example, after running your query, the above Person table should have the following rows:\n",
    "\n",
    "-- +----+------------------+\n",
    "-- | Id | Email            |\n",
    "-- +----+------------------+\n",
    "-- | 1  | john@example.com |\n",
    "-- | 2  | bob@example.com  |\n",
    "-- +----+------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10951a8-ed95-4159-bf8b-3d01c1f398bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n| Id|           Email|\n+---+----------------+\n|  1|john@example.com|\n|  2| bob@example.com|\n|  3|john@example.com|\n+---+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CreateDataFrame\").getOrCreate()\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    (1, \"john@example.com\"),\n",
    "    (2, \"bob@example.com\"),\n",
    "    (3, \"john@example.com\")\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "columns = [\"Id\", \"Email\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f61e729-7a52-4d48-8d51-f2be4e1c77a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9379c92-b942-4bae-9501-2874b525d493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---+\n| Id|           Email|row|\n+---+----------------+---+\n|  2| bob@example.com|  1|\n|  1|john@example.com|  1|\n|  3|john@example.com|  2|\n+---+----------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "windows=Window.partitionBy(\"Email\").orderBy(\"Id\")\n",
    "df=df.withColumn(\"row\",row_number().over(windows))\n",
    "# Filter to keep only the first occurrence of each Email\n",
    "#df = df.filter(\"row = 1\").drop(\"row\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e593db32-d6cb-4c06-9e43-06a4f3f8e58a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## -- Write a SQL query to find all duplicate emails in a table named Person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0bf940-efd7-4969-8505-d802327561e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n|           Email|count|\n+----------------+-----+\n|john@example.com|    2|\n+----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Email\").agg(count(\"*\").alias(\"count\")).filter(col(\"count\")>1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d879a955-e108-4b66-b88d-c9d8bfeb26cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 4\n",
    "-- Select all employee's name and bonus whose bonus is < 1000.\n",
    "\n",
    "-- Table:Employee\n",
    "\n",
    "\n",
    "-- +-------+--------+-----------+--------+\n",
    "-- | empId |  name  | supervisor| salary |\n",
    "-- +-------+--------+-----------+--------+\n",
    "-- |   1   | John   |  3        | 1000   |\n",
    "-- |   2   | Dan    |  3        | 2000   |\n",
    "-- |   3   | Brad   |  null     | 4000   |\n",
    "-- |   4   | Thomas |  3        | 4000   |\n",
    "-- +-------+--------+-----------+--------+\n",
    "-- empId is the primary key column for this table.\n",
    "-- Table: Bonus\n",
    "\n",
    "\n",
    "-- +-------+-------+\n",
    "-- | empId | bonus |\n",
    "-- +-------+-------+\n",
    "-- | 2     | 500   |\n",
    "-- | 4     | 2000  |\n",
    "-- +-------+-------+\n",
    "-- empId is the primary key column for this table.\n",
    "-- Example ouput:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba7c8e5-5714-4ec6-91e9-2bb97ad61533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Employee Bonus\").getOrCreate()\n",
    "\n",
    "# Create the Employee DataFrame\n",
    "employee_data = [\n",
    "    (1, \"John\", 3, 1000),\n",
    "    (2, \"Dan\", 3, 2000),\n",
    "    (3, \"Brad\", None, 4000),\n",
    "    (4, \"Thomas\", 3, 4000)\n",
    "]\n",
    "employee_columns = [\"empId\", \"name\", \"supervisor\", \"salary\"]\n",
    "\n",
    "employee_df = spark.createDataFrame(data=employee_data, schema=employee_columns)\n",
    "\n",
    "# Create the Bonus DataFrame\n",
    "bonus_data = [\n",
    "    (2, 500),\n",
    "    (4, 2000)\n",
    "]\n",
    "bonus_columns = [\"empId\", \"bonus\"]\n",
    "\n",
    "bonus_df = spark.createDataFrame(data=bonus_data, schema=bonus_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996b422a-6f3b-4726-8452-e9f5c739153e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+------+-----+-----+\n|empId|  name|supervisor|salary|empId|bonus|\n+-----+------+----------+------+-----+-----+\n|    1|  John|         3|  1000| NULL| NULL|\n|    2|   Dan|         3|  2000|    2|  500|\n|    3|  Brad|      NULL|  4000| NULL| NULL|\n|    4|Thomas|         3|  4000|    4| 2000|\n+-----+------+----------+------+-----+-----+\n\n+-----+----+----------+------+-----+-----+\n|empId|name|supervisor|salary|empId|bonus|\n+-----+----+----------+------+-----+-----+\n|    1|John|         3|  1000| NULL| NULL|\n|    3|Brad|      NULL|  4000| NULL| NULL|\n+-----+----+----------+------+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "new_df=employee_df.join(bonus_df,employee_df.empId==bonus_df.empId,\"left\")\n",
    "new_df.show()\n",
    "new_df.filter(\"bonus<1000 & bonus is null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07dc8b29-516e-4570-ab2f-2722f0d81aa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Question 15\n",
    "**-- The Employee table holds all employees including their managers. \n",
    "-- Every employee has an Id, and there is also a column for the manager Id.\n",
    "\n",
    "\n",
    "-- Given the Employee table, write a SQL query that finds out employees who earn more than their managers. \n",
    "-- For the above table, Joe is the only employee who earns more than his manager.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c771eb-9aa2-4fea-b7df-f3b87b7448a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Self Join Example\").getOrCreate()\n",
    "\n",
    "# Create the Employee DataFrame\n",
    "employee_data = [\n",
    "    (1, \"John\", 3, 1000),\n",
    "    (2, \"Dan\", 3, 5800),\n",
    "    (3, \"Brad\", None, 4000),\n",
    "    (4, \"Thomas\", 3, 1000)\n",
    "]\n",
    "employee_columns = [\"empId\", \"name\", \"supervisor\", \"salary\"]\n",
    "\n",
    "employee_df = spark.createDataFrame(data=employee_data, schema=employee_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db78e1c6-593b-4e53-a5cb-60c390076be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+-----+----+----------+------+\n|empId|name|supervisor|salary|empId|name|supervisor|salary|\n+-----+----+----------+------+-----+----+----------+------+\n|    2| Dan|         3|  5800|    3|Brad|      NULL|  4000|\n+-----+----+----------+------+-----+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_df.alias(\"e1\").join(employee_df.alias(\"e2\"),col(\"e1.supervisor\")==col(\"e2.empId\"),\"left\").filter(\"e1.salary>e2.salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fb2a39-0306-49d3-bda8-26374a893d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write a query to return the list of customers NOT referred by the person with id '2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d84133-4284-4d70-bd7e-0d4b7207227c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n| id|name|referee_id|\n+---+----+----------+\n|  1|Will|      NULL|\n|  2|Jane|      NULL|\n|  3|Alex|         2|\n|  4|Bill|      NULL|\n|  5|Zack|         1|\n|  6|Mark|         2|\n+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Customer Table Example\").getOrCreate()\n",
    "\n",
    "# Data for the customer table\n",
    "customer_data = [\n",
    "    (1, \"Will\", None),\n",
    "    (2, \"Jane\", None),\n",
    "    (3, \"Alex\", 2),\n",
    "    (4, \"Bill\", None),\n",
    "    (5, \"Zack\", 1),\n",
    "    (6, \"Mark\", 2)\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "customer_columns = [\"id\", \"name\", \"referee_id\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "customer_df = spark.createDataFrame(data=customer_data, schema=customer_columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "customer_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9ce48d-7c7f-4f5a-a070-eb13e5c3d76e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n| id|name|referee_id|\n+---+----+----------+\n|  1|Will|      NULL|\n|  2|Jane|      NULL|\n|  4|Bill|      NULL|\n|  5|Zack|         1|\n+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "customer_df.filter(\"referee_id <>2 or referee_id is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf148711-89b5-4024-81b4-69d31a1c7628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f456c3-bdc6-465a-b539-935bb860b5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write an SQL query to find the team size of each of the employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d042e90-6996-4a5a-977c-671833ed2820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n|employee_id|team_id|\n+-----------+-------+\n|          1|    101|\n|          2|    102|\n|          3|    101|\n|          4|    103|\n|          5|    102|\n+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_data = [\n",
    "    (1, 101),\n",
    "    (2, 102),\n",
    "    (3, 101),\n",
    "    (4, 103),\n",
    "    (5, 102)\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "employee_columns = [\"employee_id\", \"team_id\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "employee_df = spark.createDataFrame(data=employee_data, schema=employee_columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dac0bdb-87ff-4ded-966a-778c17785920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n|team_id|team_size|\n+-------+---------+\n|    101|        2|\n|    102|        2|\n|    103|        1|\n+-------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df= employee_df.groupBy(\"team_id\").agg(count(\"*\").alias(\"team_size\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf754b0-c28f-43b2-9f1f-ef1e9a19df23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n|employee_id|team_size|\n+-----------+---------+\n|          1|        2|\n|          2|        2|\n|          3|        2|\n|          4|        1|\n|          5|        2|\n+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "new_em=employee_df.join(new_df,employee_df.team_id==new_df.team_id,\"left\").select(\"employee_id\",\"team_size\")\n",
    "new_em.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8392ad5e-ff5f-42da-9a15-a20a62bcbadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Write an SQL query to report the distinct titles of the kid-friendly movies streamed in June 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be137d1-a638-4d59-9b92-480704021ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TVProgram Table:\n+----------------+----------+----------+\n|    program_date|content_id|   channel|\n+----------------+----------+----------+\n|2020-06-10 08:00|         1|LC-Channel|\n|2020-05-11 12:00|         2|LC-Channel|\n|2020-05-12 12:00|         3|LC-Channel|\n|2020-05-13 14:00|         4| Disney Ch|\n|2020-06-18 14:00|         4| Disney Ch|\n|2020-07-15 16:00|         5| Disney Ch|\n+----------------+----------+----------+\n\nContent Table:\n+----------+--------------+------------+------------+\n|content_id|         title|Kids_content|content_type|\n+----------+--------------+------------+------------+\n|         1|Leetcode Movie|           N|      Movies|\n|         2| Alg. for Kids|           Y|      Series|\n|         3| Database Sols|           N|      Series|\n|         4|       Aladdin|           Y|      Movies|\n|         5|    Cinderella|           Y|      Movies|\n+----------+--------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"TVProgram and Content Tables\").getOrCreate()\n",
    "\n",
    "# Data for TVProgram table\n",
    "tv_program_data = [\n",
    "    (\"2020-06-10 08:00\", 1, \"LC-Channel\"),\n",
    "    (\"2020-05-11 12:00\", 2, \"LC-Channel\"),\n",
    "    (\"2020-05-12 12:00\", 3, \"LC-Channel\"),\n",
    "    (\"2020-05-13 14:00\", 4, \"Disney Ch\"),\n",
    "    (\"2020-06-18 14:00\", 4, \"Disney Ch\"),\n",
    "    (\"2020-07-15 16:00\", 5, \"Disney Ch\")\n",
    "]\n",
    "tv_program_columns = [\"program_date\", \"content_id\", \"channel\"]\n",
    "\n",
    "# Create TVProgram DataFrame\n",
    "tv_program_df = spark.createDataFrame(data=tv_program_data, schema=tv_program_columns)\n",
    "\n",
    "# Data for Content table\n",
    "content_data = [\n",
    "    (1, \"Leetcode Movie\", \"N\", \"Movies\"),\n",
    "    (2, \"Alg. for Kids\", \"Y\", \"Series\"),\n",
    "    (3, \"Database Sols\", \"N\", \"Series\"),\n",
    "    (4, \"Aladdin\", \"Y\", \"Movies\"),\n",
    "    (5, \"Cinderella\", \"Y\", \"Movies\")\n",
    "]\n",
    "content_columns = [\"content_id\", \"title\", \"Kids_content\", \"content_type\"]\n",
    "\n",
    "# Create Content DataFrame\n",
    "content_df = spark.createDataFrame(data=content_data, schema=content_columns)\n",
    "\n",
    "# Display the DataFrames\n",
    "print(\"TVProgram Table:\")\n",
    "tv_program_df.show()\n",
    "\n",
    "print(\"Content Table:\")\n",
    "content_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3fe8e0-abcd-4055-9780-18e22a76b732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n|    program_date|  title|\n+----------------+-------+\n|2020-06-18 14:00|Aladdin|\n+----------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "tv_program_df.join(content_df,tv_program_df.content_id==content_df.content_id,\"left\").\\\n",
    "  filter(\"Kids_content='Y'and program_date>'2020-05-31' and program_date<'2020-06-30'\").select(\"program_date\",\"title\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b143a9-d55f-4796-a80c-086a126e7e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " Write an SQL query that reports the first login date for each player.\n",
    "\n",
    "-- The query result format is in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400c1cd4-18c8-4195-8769-6739da34ccd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity Table:\n+---------+---------+----------+------------+\n|player_id|device_id|event_date|games_played|\n+---------+---------+----------+------------+\n|        1|        2|2016-03-01|           5|\n|        1|        2|2016-05-02|           6|\n|        2|        3|2017-06-25|           1|\n|        3|        1|2016-03-02|           0|\n|        3|        4|2018-07-03|           5|\n+---------+---------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Activity Table\").getOrCreate()\n",
    "\n",
    "# Data for Activity table\n",
    "activity_data = [\n",
    "    (1, 2, \"2016-03-01\", 5),\n",
    "    (1, 2, \"2016-05-02\", 6),\n",
    "    (2, 3, \"2017-06-25\", 1),\n",
    "    (3, 1, \"2016-03-02\", 0),\n",
    "    (3, 4, \"2018-07-03\", 5)\n",
    "]\n",
    "activity_columns = [\"player_id\", \"device_id\", \"event_date\", \"games_played\"]\n",
    "\n",
    "# Create Activity DataFrame\n",
    "activity_df = spark.createDataFrame(data=activity_data, schema=activity_columns)\n",
    "\n",
    "# Display the Activity DataFrame\n",
    "print(\"Activity Table:\")\n",
    "activity_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007d7d30-e8d7-45bb-8342-16e601be05eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"player_id\").orderBy(\"event_date\")\n",
    "activity_df_with_row = activity_df.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "result_df = activity_df_with_row.filter(\"row_number = 1\").drop(\"row_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2616f964-79ae-4844-9263-6478b3cddfea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_pratice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}